{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdA3DxSWF6pBeFxES+gBoM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VK00493839/Data_Science/blob/master/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_FZZtn29xbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp1cREqP-PfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string = tf.Variable(\"this is a string\", tf.string)\n",
        "number = tf.Variable(13,tf.int16)\n",
        "floating = tf.Variable(1.03,tf.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnkjDyhU-qQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rank1_tensor = tf.Variable([\"test1\"], tf.string)\n",
        "rank2_tensor = tf.Variable([[\"test1\",'OK','Hello'], [\"test2\",'OK','Hello']], tf.string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INatvi7s-6v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.rank(rank1_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo6JSJTA-9_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rank2_tensor.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAOI6KdG_WDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor1 = tf.ones([2,2,3])\n",
        "tensor2 = tf.reshape(tensor1, [3,2,2])\n",
        "tensor3 = tf.reshape(tensor1, [6,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7vL8G__y4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o2sbx3nNnk9",
        "colab_type": "text"
      },
      "source": [
        "#Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiOJuU8KATlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves import urllib\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r21PqHhlDoXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xACr3YuDDq_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOp_lukHEiyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgS2_dsRErZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dftrain.loc[0], y_train.loc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIQfdBP4E4Vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i0uALiDFJ5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu7lP68VFLlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.age.hist(bins=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D977jBYmFPZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.sex.value_counts().plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ita-Lk3Fhek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain['class'].value_counts().plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t11zgr28Fo2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_wuzmZLF9k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfeval.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xr58jtNGJs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
        "NUMERICAL_COLUMNS = ['age','fare']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = dftrain[feature_name].unique()\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERICAL_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebKL_G5sHw9g",
        "colab_type": "text"
      },
      "source": [
        "Training prcoess with epochs with batch size of 32. If we have 10 epochs our model will see the same dataset 10 times. since we need to feed our data in batches and multiple times we need to create something called input function. The input function simply defines how our dataset will be converted into branches at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXVwslmzIPiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_input_function(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df),label_df))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    return ds\n",
        "  return input_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCT43wCxJKew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_fn = make_input_function(dftrain, y_train)\n",
        "eval_input_fn = make_input_function(dfeval, y_eval, num_epochs=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byumjUR6JfBr",
        "colab_type": "text"
      },
      "source": [
        "Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNTJSAFeJPSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "\n",
        "linear_est.train(train_input_fn)\n",
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "clear_output()\n",
        "print(result['accuracy'])\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnd0AqGpKJct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "print(result[10])\n",
        "print(result[10]['probabilities']) # ['prob of survived','prob of not survived'] of record 10 in eval_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QERlF3I8NiqD",
        "colab_type": "text"
      },
      "source": [
        "#Classfication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQf5WyUoNj8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB3u9cL5N19i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CSV_COLUMN_NAMES = ['SepalLength','SepalWidth','PetalLength','PetalWidth','Species']\n",
        "SPECIES = ['Setosa','Versicolor','Viginica']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJb1xz5OKDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = tf.keras.utils.get_file('iris_training.csv','https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv')\n",
        "test_path = tf.keras.utils.get_file('iris_test.csv','https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv')\n",
        "\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xp63AIQO1aR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZHp_CrwO_Sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL2Tk-wLPBJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrzohiVOPEZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hvgOM1iPW8X",
        "colab_type": "text"
      },
      "source": [
        "Input function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n11oV0XPRJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input function\n",
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "  # shuffle and repeat if you are in training mode\n",
        "  if training:\n",
        "    ds = ds.shuffle(1000).repeat()\n",
        "  return ds.batch(batch_size)\n",
        "\n",
        "\n",
        "# Feature Columns\n",
        "my_feature_columns = []\n",
        "for key in train.keys():\n",
        "  my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
        "print(my_feature_columns)\n",
        "\n",
        "# Building the model\n",
        "\n",
        "# 1) DNNClassifier(Deep Neural Network)\n",
        "# 2) LinearClassifier\n",
        "classifier = tf.estimator.DNNClassifier(feature_columns=my_feature_columns,hidden_units=[30,10],n_classes=3)\n",
        "\n",
        "classifier.train(input_fn=lambda: input_fn(train, train_y, training=True), steps=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTT2paajSgIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_result = classifier.evaluate(input_fn=lambda: input_fn(test, test_y, training=False))\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}'.format(**eval_result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3uAbxhPIcq",
        "colab_type": "text"
      },
      "source": [
        "# Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyeW-MpFT4gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn(features, batch_size=256):\n",
        "  return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength','SepalWidth','PetalLength','PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print('Please type numeric values as prompted')\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid:\n",
        "    val = input(feature + ': ')\n",
        "    if not val.isdigit():\n",
        "      valid = False\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
        "for pred_dict in predictions:\n",
        "  print(pred_dict)\n",
        "  class_id = pred_dict['class_ids'][0]\n",
        "  probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "  print('Prediction is \"{}\" ({:.1f}%)'.format(SPECIES[class_id], 100 * probability))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CgdvTAhxIS3",
        "colab_type": "text"
      },
      "source": [
        "#Clustering\n",
        "\n",
        "Basic steps for Algorithm K-Means\n",
        "\n",
        "1) Randomly pick K points to place K centroids\n",
        "2) Assign all of the data points to centroids by distance. The closest centroid to a point is the one it is assigned to\n",
        "3) Average of all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n",
        "4) Reassign every point once again to the closest centroid.\n",
        "5) Repeat steps 3-4 until no point chnages with centroid it belongs to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHMblG1ybnn",
        "colab_type": "text"
      },
      "source": [
        "#Hidden Markov Models\n",
        "\n",
        "The Hidden Markov Model is a finite set of states each of which is associated with a (generally multi-dimensional) probability distribution. Transistions among the states are governerd by a set of probabilities called Transition Probabilities.\n",
        "\n",
        "A hidden markov model works with probabilities to predict future events or states. In this section we will learn how to create a hidden markov model that can predict the weather.\n",
        "\n",
        "#States:\n",
        "These states could be something like 'warm' and 'cold', 'high' and 'low', 'red','green' and 'blue'. These states are hidden with the model we do not see them directly.\n",
        "\n",
        "#Observations:\n",
        "Each state has a particular outcome or observation associated with it based on a probability distribution. Eg: On a hot day Vinay has a 80% chance of being happy and 20% chance of being sad.\n",
        "\n",
        "#Transitions:\n",
        "Each state will have a probability defining the likelyhood of transitioning to a different state. Eg: a cold day has a 30% chance of being followed by a hot day and 70% chance of being followed by another cold day.\n",
        "\n",
        "To create a hidden markov model we need:\n",
        "a) states\n",
        "b) observation distribution\n",
        "c) Transition distribution\n",
        "\n",
        "https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_qmkFvSVwxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKHDN-No2sqP",
        "colab_type": "text"
      },
      "source": [
        "1) cold day endoded as 0 and hot day as 1\n",
        "2) First day in our sequence has an 80% chance of being cold\n",
        "3) Cold day has 30% chance of being followed by a hot day\n",
        "4) Hot day has 20% chance of being followed by a cold day\n",
        "5) On each day the temperature is normally distributed with mean and standard devaiation 0 and 5 on a cold day and mean and standard deviation 15 and 10 on a hot day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEkN86GW2iZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfd = tfp.distributions\n",
        "\n",
        "initial_distribution = tfd.Categorical(probs=[0.8, 0.2]) # refer point 2\n",
        "\n",
        "transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
        "                                                 [0.2, 0.8]]) # refer point 3 a4d 4\n",
        "\n",
        "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.]) # refer point 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrNCO9VB3lJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tfd.HiddenMarkovModel(\n",
        "    initial_distribution=initial_distribution,\n",
        "    transition_distribution=transition_distribution,\n",
        "    observation_distribution=observation_distribution,\n",
        "    num_steps=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHuoLNc335P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The expected temperatures for each day are given by:\n",
        "\n",
        "mean = model.mean()  # shape [7], elements approach 9.0\n",
        "\n",
        "# The log pdf of a week of temperature 0 is:\n",
        "\n",
        "print(model.log_prob(tf.zeros(shape=[7])))\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  print(mean.numpy()) # print tempeartures for 7 days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZllULQd47Ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSm1kwmH5lnA",
        "colab_type": "text"
      },
      "source": [
        "#Neural Networks with TensorFlow\n",
        "\n",
        "N = F(Summation(w*x)+b)\n",
        "For each layer one bias will be there. Function is any activation function.\n",
        "\n",
        "If we apply sigmoid activation function at output and previous hidden layer has two neurons then: \n",
        "Output=sigmoid(N1*W1 + N2*W2 + b)\n",
        "\n",
        "Activation functions:\n",
        "a) RELU (Rectified Linear Unit) (all -ve values treated as 0 and for +ve values it took the same number)\n",
        "b) Tanh (Hyperbolic Tangent) (compress values between -1 to 1)\n",
        "c) Sigmoid (compress values between 0 to 1)\n",
        "\n",
        "# Loss/Cost function\n",
        "As we know our neural network feeds information through the layers until it eventually reaches an output layer. This layer contains the results that we look at to determine the prediction from your network. In the training phase it is likely your network will make many mistakes and poor predictions. In fact at the start of training your network doesn't know anything(it has random weights and biases).\n",
        "\n",
        "We need some way of evaluating if  the network is doing well and how well it is doing. For your training data we have the features (input) and the labels (expected output) because of this we can compare the output from your network to the expected output. Based on the difference between these values we can determine if your network has done a good job or poor job. If the network has done a good job we'll make minor changes to the weights and biases if it has done a poor job your changes may be drastic. \n",
        "\n",
        "So this is where the cost/loss function comes in. These functions responsible for determining how well the network did. We pass it to the output and the expected output and it returns to us some value representing the cost/loss of the network. This effectively make the networks job to optimize this cost function, trying to make it as low as possible.\n",
        "\n",
        "Some common loss/cost functions:\n",
        "a) Mean Squared Error\n",
        "b) Mean Absolute Error\n",
        "c) Hinge Loss\n",
        "\n",
        "# Gradient Descent\n",
        "Gradient descent and back propagation are closely related. Gradient Descent is the aalgorithm used to find the optimal parameters (weights and biases) for our network. While back propagation is the process of calculating the gradinet that is used in the gradient descent step.\n",
        "\n",
        "# Optmizer\n",
        "You may sometimes see the term optmizer/optimization function. This is simply the function that implements back propagation algorithm described above. Here's a list of few common ones:\n",
        "a) Gradinet Descent\n",
        "b) Stochastic Gradient Descent\n",
        "c) Mini-Batch Gradient Descent\n",
        "d) Momentum\n",
        "e) Nesterov Accelerated Gradient\n",
        "\n",
        "https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF_IjbHb5otW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-pQRhnHDyoX",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Fashion Dataset\n",
        "This dataset includes 60,000 images for training and 10,000 for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM3201eQDsH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_minist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_minist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQsgIHKdEQ-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images.shape # 60k images that are made up of (28 X 28) = 784 pixels\n",
        "type(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TY1F0GqEWMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images[0,23,23]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTSBo2YwFGPN",
        "colab_type": "text"
      },
      "source": [
        "Our pixel values are between 0 and 255, 0 being black and 255 being white. This means we have grayscale image as there are no color channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZVlyuUREla9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVqRJ1zYFbmj",
        "colab_type": "text"
      },
      "source": [
        "Our labels are intergers ranging from 0 - 9. Each integer represents a specific article of clothing. We'll create an array of label names indicate which is which"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCOPLJ4wFXN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM3PyR6HF_mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg1Q8n5CGmZG",
        "colab_type": "text"
      },
      "source": [
        "Data PreProcessing\n",
        "\n",
        "The last step before creating our model is to preprocess our data. This simply means applying some prior transformations to your data before feeding it in the model. In this case we'll simply scale all of our greyscale pixel values (0-255) to be between 0 and 1. We can do this by dividing each value in the training and testing by 255.0. We do this because smaller values will make it easier for the model to process your values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lutMhiQGLg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6_HZ-OwHYlk",
        "colab_type": "text"
      },
      "source": [
        "#Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glErXyv9HXwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)), # input layer (1)\n",
        "    keras.layers.Dense(128,activation='relu'), # hidden layer (2)\n",
        "    keras.layers.Dense(10,activation='softmax') # output layer (3)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVoqU6EqNcsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2FDVgSNN44L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c90K0nQBOI-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=1)\n",
        "print('Test Accuracy: ',test_acc) # overfitting beacause test acc less than train acc(tune parameters to overcome this problem) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2ryLPVhPCVO",
        "colab_type": "text"
      },
      "source": [
        "#Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXLRLBooOmPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_images) # (60000,28,28)\n",
        "print(predictions,'\\n')\n",
        "print(predictions[0],'\\n')\n",
        "print(np.argmax(predictions[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUid8jsAPrQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(class_names[np.argmax(predictions[0])]) # predicting the same items\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWuMD737QScJ",
        "colab_type": "text"
      },
      "source": [
        "# Verifying predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SabnG4xQLqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLOR = 'white'\n",
        "plt.rcParams['text.color'] = COLOR\n",
        "plt.rcParams['axes.labelcolor'] = COLOR\n",
        "\n",
        "def predict(model, image, correct_label):\n",
        "  class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "  prediction = model.predict(np.array([image]))\n",
        "  predicted_class = class_names[np.argmax(prediction)]\n",
        "\n",
        "  show_image(image, class_names[correct_label], predicted_class)\n",
        "\n",
        "def show_image(img, label, guess):\n",
        "  plt.figure()\n",
        "  plt.title(\"Expected: \" + label)\n",
        "  print(\"Expected: \" + label)\n",
        "  print(\"Guess: \" + guess)\n",
        "  plt.xlabel(\"Guess: \" + guess)\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "  plt.colorbar()\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "def get_number():\n",
        "  while True:\n",
        "    num = input(\"pick a number: \")\n",
        "    if num.isdigit():\n",
        "      num = int(num)\n",
        "      if 0 <= num <= 1000:\n",
        "        return int(num)\n",
        "      else:\n",
        "        print(\"Try Agian..\")\n",
        "\n",
        "\n",
        "num = get_number()\n",
        "image = test_images[num]\n",
        "label = test_labels[num]\n",
        "predict(model, image, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQsAFhFTSci",
        "colab_type": "text"
      },
      "source": [
        "# Deep Computer Vision\n",
        "In this guide we will learn how to perform image classification and object detection/recognition using Deep Computer Vision with something called a Convolutional Neural Network.\n",
        "\n",
        "The goal of our CNN will be to classify and detect images or specific objects from within the image. We'll be using image data as our features and a label for those images as our label or output.\n",
        "\n",
        "We already know how Neural Networks work so we can skip through the basics and move right into the following concepts:\n",
        "a) Image Data\n",
        "b) Convolutional Layer\n",
        "c) Pooling Layer\n",
        "d) CNN Architectures\n",
        "\n",
        "The main difference we are about to see in these types of neural networks are the layers that make them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OQ9e7HPLIcv",
        "colab_type": "text"
      },
      "source": [
        "# Image Data\n",
        "\n",
        "So far we have dealt with straight forward data that has 1 or 2 dimesnions. Now we are about to deal with Image data made up of 3 dimensions. These 3 dimensions are:\n",
        "a) Image Height\n",
        "b) Image Width\n",
        "c) Color channels\n",
        "\n",
        "The number of color channels represents the depth of an image corelates to the colors used in it. For eg., image with 3 channels made up of rgb(red, green, blue) pixels. So for each channel we have 3 numeric values in the range of 0-255 that define its color. For an image of color depth 1 we would likely have a greyscale image one value defining each pixel, again in the range of 0-255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOm9EGepMghz",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "Each CNN is made up of one or many convolutional layers. These layers are different than the dense layers we have seen previously. Their goal is to find the patterns from within images that can be used to classify the image or parts of it. But this may sound familiar to what our densely connected Neural Network in the previuos section was doing.\n",
        "\n",
        "\n",
        "The fundamental difference between a dense layer and a convolutional layer is that the dense layer detect patterns globally while convolutional layer detects patterns locally. when we have a densely connected layer each node in that layer sees all the data from the previous layer. This means that this layer is looking ALL of the information and is only capable of analyzing the data in a global capacity. Our convolutional layer however will not be densely connected, this means it can detect local patterns using part of the input data to that layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to2GX3pdOQSp",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Convolutional Layers\n",
        "\n",
        "Even the basic example we'll use in this guid will be made up of 3 convolutional layers. These layers work together by increasing complexity and abstraction at each subsequent layer. The first layer might be responsible for picking up edges and short lines, while the second layer will take as input these lines and start forming shapes or polygons. Finally the last layer might take these shapes and detrmine which combinations make up a specific image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzl1P61O3yq",
        "colab_type": "text"
      },
      "source": [
        "# Feature Maps\n",
        "\n",
        "This term simply stands for a 3D tensor with two special axes(width and height) and one depth axis. Our convolutional layers take feature maps as their input and return a new feature map that represents the presence of specific filters from the previous feature map. These are what we call response maps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEPMlC1PVJK",
        "colab_type": "text"
      },
      "source": [
        "#Layer Parameters\n",
        "A convolutional network has 2 key parameters\n",
        "\n",
        "# Filters\n",
        "\n",
        "A filter is a m X n pattern of pixels that we are looking for an image. The number of filters in a convolutional layer represents how many patterns each layer is looking for and what the depth of our response map will be. If we are looking for 32 different patterns/filters than our output feature map (aka the response map) will have a depth of 32 layers of depth will be a matrix of some size containing values including if the filter was present at that location or not.\n",
        "\n",
        "# Sample Size\n",
        "\n",
        "This isn't really the best term to describe this but each convolutional layer is going to examine n X m blocks of pixels in each imgae. Typically we'll consider 3X3 or 5X5 blocks. In the example above 3X3 'Smaple size'. This size will be the same as the size of our filter. Our layers work by sliding these filters of nXm pixels over every possible position in our image and populating a new feature map/response map indicating whether the filter is present at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3AUENIaSkpO",
        "colab_type": "text"
      },
      "source": [
        "# After Layers we are going to pool those filter values\n",
        "\n",
        "min/max/average are differnet pooling techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrFF_PrhS2zl",
        "colab_type": "text"
      },
      "source": [
        "#https://www.tensorflow.org/tutorials/images/cnn\n",
        "\n",
        "CIFAR Image Dataset having 60,000 32X32 color images with 6000 images of each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzfZKZ62SJwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkkg43wTTaba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and split dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsBlCTKtTkIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3_i93STpp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets look at one image\n",
        "IMG_INDEX = 1\n",
        "\n",
        "plt.imshow(train_images[IMG_INDEX], cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofb_phimUQOV",
        "colab_type": "text"
      },
      "source": [
        "# CNN Architecture\n",
        "\n",
        "A common architecture for a CNN is stack of Conv2D and MaxPooling2D layers followed by a few densely connected layers. To idea is that the stack of convolutional and maxpooling layers extract the features from the image. Then these features are flattened and fed to densely connected layers that determine the class of an image based on the presence of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugYGskmULRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtyP3me_U-XF",
        "colab_type": "text"
      },
      "source": [
        "#Layer1\n",
        "Input shape of our data will be 32,32,3 and we will process 32 filters of size 3x3 over our input data. We will also apply the activation function to the output of each convolutional operation.\n",
        "\n",
        "#Layer2\n",
        "This layer will perform max pooling operation using 2x2 samples and a stride of 2\n",
        "\n",
        "# Other Layers\n",
        "The next step of layers do very similar things but take as input the feature map from previous layer. They also increase the frequencies from 32 to 64. We can do this as our data shrinks in special dimensions as it passed through the layers, meaning we can afford computationally to ad more depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DUm1BUaU5uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN6wPgJQWPlk",
        "colab_type": "text"
      },
      "source": [
        "# Adding Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GarAvsQJWEbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten()) #(4,4,64)=>4*4*64=1024 \n",
        "model.add(layers.Dense(64, activation='relu')) # adding dense layer 64 with activation function relu\n",
        "model.add(layers.Dense(10)) # adding dense layer 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YIuN8_wWVGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8pqTqRtWufs",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsTPZzyzWW1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=4, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jqRTx_FXMmD",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMZDWT4iW12L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptaqprUWXVqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JrRJdD2YuqR",
        "colab_type": "text"
      },
      "source": [
        "#Working with Small Datasets\n",
        "IN the situation where you dont have millions of images it is difficult to train a CNN from scratch that performs very well. This is why we will learn about a few techniques we can use to train CNN's on small datasets of just a few thousand images.\n",
        "\n",
        "# Data Augmentation\n",
        "To avoid overfitting and create a larger dataset from a similar one we can use a technique called data augmentation. This is simply performing random transformations on our images so that our model can generalize better. These transformations can be things like Compressions, rotations, stretches and even color changes. Fortunately keras can help us to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iexZoIqYm1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# creates a data generator object that transforms images\n",
        "datagen = ImageDataGenerator(rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')\n",
        "\n",
        "# pick an image to transform\n",
        "test_img = train_images[4]\n",
        "img = image.img_to_array(test_img) # convert image to numpy array\n",
        "img = img.reshape((1,) + img.shape) # reshapes image\n",
        "\n",
        "i=0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):\n",
        "  plt.figure(i)\n",
        "  plot = plt.imshow(image.img_to_array(batch[0]))\n",
        "  i += 1\n",
        "  if i > 4:\n",
        "    break\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNkGRg3vbH4z",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained Models\n",
        "you would have noticed that the model above takes a few minutes to train in the Notebook and only gives an accuracy of 70%. This is okay but surely there is a way to improve on this.\n",
        "\n",
        "In this section, we will talk about using a pretrained CNN as apart of our own custom network to improve the accuracy of our model. We know that CNN's alone (with no dense layers) dont do anything other than map the presence of features from our input. This means we can use a pretrained CNN, one trained on millions of images, as the start of our model. This will allow us to have a very good convolutional base before adding our own dense layered classifier at the end. Infact by using this technique, we can train a very good classifier for a relatively small dataset(<10,000 images). This is because the convnet already has a very good idea of what features to look for in an image and can find them very effectively. So if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.\n",
        "\n",
        "# Fine Tuning\n",
        "When we employ the technique defined above we'll often want tweak the final layers in our convolutional base to work better for our specific problem. This involves not touching or re-training the earlier layers in our convolutional base but only adjusting the final few. We do this because the first layers in our base are very good at extracting low level features like lines and edges, things that are similar for any kind of image. Where the later layers are better at picking up very specific features like shapes or even eyes. If we adjust the final layers than we can look for only features relevant to our very specific problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eyXhCgaeMce",
        "colab_type": "text"
      },
      "source": [
        "# Using a Pretrained Model\n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slQJn2fHa6pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7WUlgktetBZ",
        "colab_type": "text"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "We will load the cas_vs_dogs dataset from the module tensorflow_datasets\n",
        "\n",
        "This dataset contains (image, label) pairs where images have different dimensions and 3 color channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbAx0G-DejFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# split the data into 80% training, 10% testing, and 10% validation\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5L0gXbZfMsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_label_name = metadata.features['label'].int2str\n",
        "\n",
        "for image, label in raw_train.take(2):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP9NWnSKfntu",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRCHnQufl0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "def format_example(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13oyYPcaf1kP",
        "colab_type": "text"
      },
      "source": [
        "Apply this function to each item in the dataset using the map method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqMfz-wcf2DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWYUFA8IgG-k",
        "colab_type": "text"
      },
      "source": [
        "Lets have a look at our images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_agHqz5wgGRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image, label in train.take(2):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov1Q2calgAti",
        "colab_type": "text"
      },
      "source": [
        "Now shuffle and batch the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z42ulFAGgCh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTPdVcA6hNpX",
        "colab_type": "text"
      },
      "source": [
        "Inspect a batch of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW9NFDsphPH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_batch, label_batch in train_batches.take(1):\n",
        "   pass\n",
        "\n",
        "image_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFOKUqrtgoaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for img, label in raw_train.take(2):\n",
        "  print('Original Shape: ', img.shape)\n",
        "\n",
        "for img, label in train.take(2):\n",
        "  print('New Shape: ', img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MyZQwhg9EY",
        "colab_type": "text"
      },
      "source": [
        "#Create the base model from the pre-trained convnets\n",
        "You will create the base model from the MobileNet V2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help us classify cats and dogs from our specific dataset.\n",
        "\n",
        "First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
        "\n",
        "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnCzNTWng1IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtbS73TphWCs",
        "colab_type": "text"
      },
      "source": [
        "This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features. See what it does to the example batch of images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap5glsarhSqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_batch = base_model(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HCkVTKNhZBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMciw6ouhsnR",
        "colab_type": "text"
      },
      "source": [
        "#Feature extraction\n",
        "\n",
        "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "#Freeze the convolutional base\n",
        "\n",
        "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL6VQCAUhbwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2GriPTrh03j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqyLNfQ_h9X1",
        "colab_type": "text"
      },
      "source": [
        "#Add our classifier\n",
        "\n",
        "Now that we have our base layer setup we can add the classfifier. Insted of flattening the feature map of the base kayer we will use a global avergae pooing layer that will average the entire 5x5 area of each 2D feature map and return to us a single 1280 element vector per filter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suCijswvh2ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10dT_xyHivo6",
        "colab_type": "text"
      },
      "source": [
        "Finally we will add the prediction layer that will be a single dense neuron. We can do this because we only have two classes to predict for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO_WtZ5qiD5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_layer = tf.keras.layers.Dense(1)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOnaVjB2i_dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xr4VOSIkBRI",
        "colab_type": "text"
      },
      "source": [
        "#compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av1cPQ-CkaqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiXCXBWVjfNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzr3mlgaj9PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiKxDewTkMz-",
        "colab_type": "text"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzbD8TtYkKxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_epochs = 3\n",
        "validation_steps=20\n",
        "\n",
        "loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dib1uTUxkd6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_B1t0rkh7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_batches,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=validation_batches)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJVKwqoktoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('dogs_vs_cats.h5')\n",
        "new_model = tf.keras.models.load_model('dogs_vs_cats.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}